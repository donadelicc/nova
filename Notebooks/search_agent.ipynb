{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install:\n",
    "\n",
    "Python packages used in this notebook:\n",
    "- pandas\n",
    "- os\n",
    "- langchain-openai\n",
    "- dotenv\n",
    "- langchain-community\n",
    "- langchain\n",
    "- langchain-core\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AZURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"3293d148178a4cf39f5b3026cb621eb1\" \n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://fish-health-bsc-resource-aiservices1116524949.openai.azure.com/\"\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "     openai_api_version=\"2023-07-01-preview\",\n",
    "     azure_deployment=\"gpt-4\",\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=\"2023-07-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPENAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET AL ENVIROMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAVILY - Built in search engine tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.timeanddate.no/vaer/norge/geilo/siste-uke',\n",
       "  'content': 'V√¶ret som var i Geilo for siste 24 timer Vis v√¶ret for: Siste 24 timer 19. mars 2024 18. mars 2024 17. mars 2024 16. mars 2024 15. mars 2024 14. mars 2024 13. mars 2024 12. mars 2024 11. mars 2024 10. mars 2024 9. mars 2024 8. mars 2024 7. mars 2024 6. mars 2024 5. mars 2024 4. mars 2024'},\n",
       " {'url': 'https://www.yr.no/nb/v√¶rvarsel/timetabell/5-25630/Norge/Buskerud/Hol/Geilo?i=2',\n",
       "  'content': '0 til 0.4. 0-0,4. 3 m/s fra s√∏rvest med vindkast p√• 9 m/s. 3 (9) Svak vind fra s√∏rvest med vindkast p√• 9 m/s. Sol opp 06:21. Sol ned 18:47. Forrige dag Neste dag. Dagens v√¶rvarsel for Geilo time for time.'},\n",
       " {'url': 'https://www.timeanddate.no/vaer/norge/geilo',\n",
       "  'content': 'V√¶ret i Geilo, Norge. V√¶ret. Tidssone. Sommertid. Sol og m√•ne. V√¶ret i dag Time for time 14-dagers varsel I g√•r/v√¶ret som var Klima (gjennomsnitt) N√•.'},\n",
       " {'url': 'https://www.yr.no/nb/v√¶rvarsel/daglig-tabell/1-112132/Norge/Buskerud/Hol/Geilo',\n",
       "  'content': 'Geilo Tettsted, Hol (Buskerud), 798 moh. Trykk for √• vise informasjon om dette stedet. ... Det ser ut til at JavaScript er skrudd av i nettleseren din. For √• f√• tilgang til alt av innhold p√• Yr anbefaler vi at du skrur p√• JavaScript. V√¶ret n√• ...'},\n",
       " {'url': 'https://www.timeanddate.no/vaer/norge/geilo/langtids',\n",
       "  'content': 'V√¶ret i verden; V√¶ret der du er; Neste 14 dager; Time for time; V√¶ret i Norge; Siste 14 dager; Klima. Sol, m√•ne og universet. Sol og M√•ne: Alle tjenester; Soloppgang og solnedgang; ... 2-ukers utvidet v√¶rmelding for Geilo, Norge. Bla mot h√∏yre for √• se mer Forhold Vindstyrke og luftfuktighet Nedb√∏r Sola; Dag Temperatur V√¶ret F√∏les ...'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()\n",
    "\n",
    "search.invoke(\"Hvordan er v√¶ret i Geilo, Norge?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a vectorestore with the Langsmith documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_functions_agent\n",
    "\n",
    "agent = create_openai_functions_agent(model, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello! How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'hi!', 'output': 'Hello! How can I assist you today?'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"hi!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'how can LangSmith help with testing'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mSkip to main contentü¶úÔ∏èüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start‚ÄãTracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps‚ÄãCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API key, and configure your\n",
      "\n",
      "about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end walkthroughs using LangSmith.LangChain Python: Docs for the Python LangChain library.LangChain Python API Reference: documentation to review the core APIs of LangChain.LangChain JS: Docs for the TypeScript LangChain libraryDiscord: Join us on our Discord to discuss all things LangChain!Contact SalesIf you're interested in\n",
      "\n",
      "LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\n",
      "\n",
      "to review the core APIs of LangChain.LangChain JS: Docs for the TypeScript LangChain libraryDiscord: Join us on our Discord to discuss all things LangChain!Contact SalesIf you're interested in enterprise security and admin features, special deployment options, or access for large teams, reach out to speak with sales.Was this page helpful?NextUser GuideIntroductionQuick StartNext StepsAdditional ResourcesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright ¬© 2024 LangChain, Inc.\u001b[0m\u001b[32;1m\u001b[1;3mLangSmith, developed by LangChain, is a platform specifically designed to help with building production-grade LLM applications. One of its features includes providing support for testing. This platform allows you to debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework. Seamless integration with LangChain, the popular open-source framework for building with LLMs, is also provided. \n",
      "\n",
      "To get started with testing in LangSmith, you can refer to the tracing and evaluation quick start guides provided in the platform's documentation. Other sections in the user guide also explain the workflows that LangSmith supports at each stage of the LLM application lifecycle.\n",
      "\n",
      "Please note that before you begin, you'll need to create an account, obtain an API key, and configure your environment.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how can langsmith help with testing?',\n",
       " 'output': \"LangSmith, developed by LangChain, is a platform specifically designed to help with building production-grade LLM applications. One of its features includes providing support for testing. This platform allows you to debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework. Seamless integration with LangChain, the popular open-source framework for building with LLMs, is also provided. \\n\\nTo get started with testing in LangSmith, you can refer to the tracing and evaluation quick start guides provided in the platform's documentation. Other sections in the user guide also explain the workflows that LangSmith supports at each stage of the LLM application lifecycle.\\n\\nPlease note that before you begin, you'll need to create an account, obtain an API key, and configure your environment.\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'current weather in √Öros, R√∏yken, Norway'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.wunderground.com/forecast/no/√•ros', 'content': 'Current Weather for Popular Cities . San Francisco, CA warning 48 ... √Öros, Viken, Norway 10-Day Weather Forecast star_ratehome. 17 ...'}, {'url': 'https://weather.com/weather/tenday/l/√Öros+Viken+Norway?canonicalCityId=89fab4fd4a4171e85b636661f200f9395bee4a1777531f2fe70171f4e2585a80', 'content': 'Be prepared with the most accurate 10-day forecast for √Öros, Viken, Norway with highs, lows, chance of precipitation from The Weather Channel and Weather.com'}, {'url': 'https://www.yr.no/en/forecast/daily-table/1-59711/Norway/Akershus/Asker/√Öros', 'content': 'Saturday 9 Mar. 5¬∞ / -1¬∞. 0 mm. 2 m/s. Open hourly forecast. Updated 22:36.How often is the weather forecast updated? Forecast as PDF Forecast as SVG. Weather forecast for √Öros for the next 10 days.'}, {'url': 'https://www.timeanddate.com/weather/norway/royken/ext', 'content': '(Weather station: Hasle - MADIS Station, Norway). See more current weather. ... See weather overview. 2 Week Extended Forecast in R√∏yken, Norway. Scroll right to see more Conditions Comfort Precipitation Sun; Day Temperature Weather Feels Like Wind Humidity Chance Amount UV Sunrise Sunset; Sun Mar 17: 36 / 14 ¬∞F: Increasing cloudiness. 35 ¬∞F:'}, {'url': 'https://weather.com/weather/today/l/√Öros+Viken+Norway?canonicalCityId=89fab4fd4a4171e85b636661f200f9395bee4a1777531f2fe70171f4e2585a80', 'content': \"Today's and tonight's √Öros, Viken, Norway weather forecast, weather conditions and Doppler radar from The Weather Channel and Weather.com\"}]\u001b[0m\u001b[32;1m\u001b[1;3mV√¶ret i √Öros, R√∏yken, Norge varierer sterkt, men for √∏yeblikket er det rundt 5¬∞ / -1¬∞. Det er forventet √• v√¶re noe overskyet. For mer detaljert informasjon om v√¶ret i √Öros, kan du bes√∏ke nettsteder som The Weather Channel, Wunderground eller yr.no.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hvordan er v√¶ret i √Öros i R√∏yken, Norge?',\n",
       " 'output': 'V√¶ret i √Öros, R√∏yken, Norge varierer sterkt, men for √∏yeblikket er det rundt 5¬∞ / -1¬∞. Det er forventet √• v√¶re noe overskyet. For mer detaljert informasjon om v√¶ret i √Öros, kan du bes√∏ke nettsteder som The Weather Channel, Wunderground eller yr.no.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Hvordan er v√¶ret i √Öros i R√∏yken, Norge?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADDING IN MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello Bob! How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'hi! my name is bob',\n",
       " 'chat_history': [],\n",
       " 'output': 'Hello Bob! How can I assist you today?'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we pass in an empty list of messages for chat_history because it is the first message in the chat\n",
    "agent_executor.invoke({\"input\": \"hi! my name is bob\", \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYour name is Bob. How can I assist you further?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='hi! my name is bob'),\n",
       "  AIMessage(content='Hello Bob! How can I assist you today?')],\n",
       " 'input': \"what's my name?\",\n",
       " 'output': 'Your name is Bob. How can I assist you further?'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"hi! my name is bob\"),\n",
    "            AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        ],\n",
    "        \"input\": \"what's my name?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEEP TRACK OF MESSAGES AUTOMATICALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello Bob! How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"hi! I'm bob\",\n",
       " 'chat_history': [],\n",
       " 'output': 'Hello Bob! How can I assist you today?'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"hi! I'm bob\"},\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
